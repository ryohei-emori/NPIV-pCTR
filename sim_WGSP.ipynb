{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":159185,"status":"ok","timestamp":1717475596565,"user":{"displayName":"R E","userId":"10479407368002365455"},"user_tz":-540},"id":"86gx5TTL86yg","outputId":"c094e45e-133b-4e5c-c1d3-0a9ee080f047"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from scipy.special import logit, expit\n","from scipy.stats import multivariate_normal, bernoulli\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import log_loss, roc_AUC_score\n","\n","def set_seed(seed=42):\n","    np.random.seed(seed)\n","\n","def generate_parameters(num_features):\n","    alpha = np.random.normal(0.1, 1, num_features)\n","    beta = np.random.normal(0.1, 1, num_features)\n","    gamma = np.random.normal(0.1, 1, num_features)\n","    return alpha, beta, gamma\n","\n","def generate_sigma(num_features):\n","    A = np.random.normal(0, 1, (num_features + 1, num_features + 1))\n","    Sigma = np.dot(A, A.T)\n","    return Sigma\n","\n","def generate_mvn_samples(n, mu, Sigma):\n","    return multivariate_normal.rvs(mean=mu, cov=Sigma, size=n)\n","\n","def sample_reparameterized_beta(mu, phi, size):\n","    mu = np.clip(mu, 0.01, 0.99)\n","    alpha = mu * phi\n","    beta_param = (1 - mu) * phi\n","    return np.random.beta(alpha, beta_param, size=size)\n","\n","def initialize_training_data(num_records_initial, num_features, Sigma, gamma, beta):\n","    X_k_list = []\n","    eta_k_list = []\n","    y_k_list = []\n","    D_k_list = []\n","\n","    while len(D_k_list) < num_records_initial:\n","        X_k_eta_k = generate_mvn_samples(num_records_initial, np.zeros(num_features + 1), Sigma)\n","        X_k = X_k_eta_k[:, :-1]\n","        eta_k = X_k_eta_k[:, -1]\n","\n","        D_k_prob = expit(np.dot(X_k, gamma) + eta_k)\n","        D_k = bernoulli.rvs(D_k_prob)\n","\n","        X_k = X_k[D_k == 1]\n","        eta_k = eta_k[D_k == 1]\n","        p_k = expit(np.dot(X_k, beta) + eta_k)\n","        y_k = bernoulli.rvs(p_k)\n","        D_k = D_k[D_k == 1]\n","\n","        X_k_list.append(X_k)\n","        eta_k_list.append(eta_k)\n","        y_k_list.append(y_k)\n","        D_k_list.append(D_k)\n","\n","    X_k = np.concatenate(X_k_list, axis=0)[:num_records_initial]\n","    eta_k = np.concatenate(eta_k_list, axis=0)[:num_records_initial]\n","    y_k = np.concatenate(y_k_list, axis=0)[:num_records_initial]\n","    D_k = np.concatenate(D_k_list, axis=0)[:num_records_initial]\n","\n","    assert len(D_k) == num_records_initial\n","\n","    return X_k, eta_k, y_k, D_k\n","\n","def train_logistic_regression(X_k, y_k):\n","    pCTR_biased = LogisticRegression()\n","    pCTR_biased.fit(X_k, y_k)\n","    return pCTR_biased\n","\n","def generate_historical_auction_data(num_auctions, num_auctioneers_per_auction, num_features, Sigma, alpha, beta, pCTR_biased):\n","    historical_data = []\n","    for _ in range(num_auctions):\n","        X_j_eta_j = generate_mvn_samples(num_auctioneers_per_auction, np.zeros(num_features + 1), Sigma)\n","        X_j = X_j_eta_j[:, :-1]\n","        eta_j = X_j_eta_j[:, -1]\n","        mu_j = expit(np.dot(X_j, alpha))\n","        bids = sample_reparameterized_beta(mu_j, 2, size=num_auctioneers_per_auction)\n","        pCTR_j = pCTR_biased.predict_proba(X_j)[:, 1]\n","        auction_scores = bids * pCTR_j\n","        j_star = np.argmax(auction_scores)\n","        p_j = expit(np.dot(X_j[j_star], beta) + eta_j[j_star])\n","        y_j = np.zeros(num_auctioneers_per_auction)\n","        y_j[j_star] = bernoulli.rvs(p_j)\n","        D_j = np.zeros(num_auctioneers_per_auction)\n","        D_j[j_star] = 1\n","        historical_data.append((y_j, X_j, bids, D_j))\n","\n","    return historical_data\n","\n","def flatten_historical_data(historical_data):\n","    y_flat = []\n","    X_flat = []\n","    bids_flat = []\n","    D_flat = []\n","    for y_j, X_j, bids, D_j in historical_data:\n","        y_flat.extend(y_j)\n","        X_flat.extend(X_j)\n","        bids_flat.extend(bids)\n","        D_flat.extend(D_j)\n","\n","    y_flat = np.array(y_flat)\n","    X_flat = np.array(X_flat)\n","    bids_flat = np.array(bids_flat)\n","    D_flat = np.array(D_flat)\n","\n","    return y_flat, X_flat, bids_flat, D_flat\n","\n","def generate_and_train():\n","    # Set the random seed for reproducibility\n","    set_seed()\n","\n","    # Define constants\n","    num_records_initial = 5000\n","    num_records_validation = 50000\n","    num_auctions = 5000\n","    num_auctioneers_per_auction = 20\n","    num_features = 25\n","\n","    # Generate parameters and Sigma\n","    alpha, beta, gamma = generate_parameters(num_features)\n","    Sigma = generate_sigma(num_features)\n","\n","    # Step 1: Initializing and generating training data\n","    X_k, eta_k, y_k, D_k = initialize_training_data(num_records_initial, num_features, Sigma, gamma, beta)\n","\n","    # Define a logistic regression model for pCTR and train it\n","    pCTR_biased = train_logistic_regression(X_k, y_k)\n","\n","    # Step 2: Generating historical auction data\n","    historical_data = generate_historical_auction_data(num_auctions, num_auctioneers_per_auction, num_features, Sigma, alpha, beta, pCTR_biased)\n","\n","    # Flatten the historical data for learning\n","    y_flat, X_flat, bids_flat, D_flat = flatten_historical_data(historical_data)\n","\n","    return y_flat, X_flat, bids_flat, D_flat\n","\n","#y_flat, X_flat, bids_flat, D_flat = generate_and_train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":384417,"status":"ok","timestamp":1716287144940,"user":{"displayName":"R E","userId":"10479407368002365455"},"user_tz":-540},"id":"WPkGptCwEmq-","outputId":"14cf7284-3ecb-4b48-d93f-5ef777a80194"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['z'] which did not match any model input. They will be ignored by the model.\n","  inputs = self._flatten_to_reference_inputs(inputs)\n"]},{"name":"stdout","output_type":"stream","text":[" 995/1000 [============================>.] - ETA: 0s - loss: 3.5410e-04\n","Epoch 1: loss improved from inf to 0.00035, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 15s 13ms/step - loss: 3.5268e-04\n","Epoch 2/30\n"," 995/1000 [============================>.] - ETA: 0s - loss: 1.9458e-05\n","Epoch 2: loss improved from 0.00035 to 0.00002, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 1.9384e-05\n","Epoch 3/30\n"," 995/1000 [============================>.] - ETA: 0s - loss: 6.5088e-06\n","Epoch 3: loss improved from 0.00002 to 0.00001, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 6.4903e-06\n","Epoch 4/30\n"," 998/1000 [============================>.] - ETA: 0s - loss: 2.7523e-06\n","Epoch 4: loss improved from 0.00001 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 2.7471e-06\n","Epoch 5/30\n"," 998/1000 [============================>.] - ETA: 0s - loss: 1.1297e-06\n","Epoch 5: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 1.1276e-06\n","Epoch 6/30\n"," 997/1000 [============================>.] - ETA: 0s - loss: 3.1834e-07\n","Epoch 6: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 3.1755e-07\n","Epoch 7/30\n"," 997/1000 [============================>.] - ETA: 0s - loss: 1.2359e-07\n","Epoch 7: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 1.2324e-07\n","Epoch 8/30\n"," 996/1000 [============================>.] - ETA: 0s - loss: 5.6574e-08\n","Epoch 8: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 12s 12ms/step - loss: 5.6425e-08\n","Epoch 9/30\n"," 999/1000 [============================>.] - ETA: 0s - loss: 1.8273e-08\n","Epoch 9: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 12s 12ms/step - loss: 1.8255e-08\n","Epoch 10/30\n"," 999/1000 [============================>.] - ETA: 0s - loss: 9.7016e-09\n","Epoch 10: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 12s 12ms/step - loss: 9.6925e-09\n","Epoch 11/30\n","1000/1000 [==============================] - ETA: 0s - loss: 1.3400e-08\n","Epoch 11: loss did not improve from 0.00000\n","1000/1000 [==============================] - 10s 10ms/step - loss: 1.3400e-08\n","Epoch 12/30\n"," 997/1000 [============================>.] - ETA: 0s - loss: 2.7487e-09\n","Epoch 12: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 2.7408e-09\n","Epoch 13/30\n"," 999/1000 [============================>.] - ETA: 0s - loss: 1.4674e-09\n","Epoch 13: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 1.4659e-09\n","Epoch 14/30\n"," 999/1000 [============================>.] - ETA: 0s - loss: 9.9725e-10\n","Epoch 14: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 9.9704e-10\n","Epoch 15/30\n"," 997/1000 [============================>.] - ETA: 0s - loss: 7.8888e-10\n","Epoch 15: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 7.8692e-10\n","Epoch 16/30\n"," 999/1000 [============================>.] - ETA: 0s - loss: 5.8416e-10\n","Epoch 16: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 5.8373e-10\n","Epoch 17/30\n"," 997/1000 [============================>.] - ETA: 0s - loss: 4.3143e-10\n","Epoch 17: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 12s 12ms/step - loss: 4.3086e-10\n","Epoch 18/30\n","1000/1000 [==============================] - ETA: 0s - loss: 2.9928e-10\n","Epoch 18: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 12s 12ms/step - loss: 2.9928e-10\n","Epoch 19/30\n"," 997/1000 [============================>.] - ETA: 0s - loss: 2.5451e-10\n","Epoch 19: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 12s 12ms/step - loss: 2.5386e-10\n","Epoch 20/30\n"," 998/1000 [============================>.] - ETA: 0s - loss: 2.9558e-10\n","Epoch 20: loss did not improve from 0.00000\n","1000/1000 [==============================] - 10s 10ms/step - loss: 2.9502e-10\n","Epoch 21/30\n","1000/1000 [==============================] - ETA: 0s - loss: 1.9138e-10\n","Epoch 21: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 1.9138e-10\n","Epoch 22/30\n"," 997/1000 [============================>.] - ETA: 0s - loss: 1.9854e-10\n","Epoch 22: loss did not improve from 0.00000\n","1000/1000 [==============================] - 11s 11ms/step - loss: 1.9832e-10\n","Epoch 23/30\n","1000/1000 [==============================] - ETA: 0s - loss: 1.5997e-10\n","Epoch 23: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 16s 16ms/step - loss: 1.5997e-10\n","Epoch 24/30\n"," 998/1000 [============================>.] - ETA: 0s - loss: 1.6259e-10\n","Epoch 24: loss did not improve from 0.00000\n","1000/1000 [==============================] - 11s 11ms/step - loss: 1.6228e-10\n","Epoch 25/30\n"," 997/1000 [============================>.] - ETA: 0s - loss: 1.2615e-10\n","Epoch 25: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 1.2578e-10\n","Epoch 26/30\n"," 999/1000 [============================>.] - ETA: 0s - loss: 1.1149e-10\n","Epoch 26: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 1.1139e-10\n","Epoch 27/30\n"," 999/1000 [============================>.] - ETA: 0s - loss: 2.8153e-10\n","Epoch 27: loss did not improve from 0.00000\n","1000/1000 [==============================] - 10s 10ms/step - loss: 2.8125e-10\n","Epoch 28/30\n"," 996/1000 [============================>.] - ETA: 0s - loss: 1.0046e-10\n","Epoch 28: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 1.0041e-10\n","Epoch 29/30\n"," 999/1000 [============================>.] - ETA: 0s - loss: 1.0016e-10\n","Epoch 29: loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/naive_checkpoint\n","1000/1000 [==============================] - 13s 13ms/step - loss: 1.0007e-10\n","Epoch 30/30\n"," 998/1000 [============================>.] - ETA: 0s - loss: 1.2151e-10\n","Epoch 30: loss did not improve from 0.00000\n","1000/1000 [==============================] - 11s 11ms/step - loss: 1.2128e-10\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x787109bdf220>"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["### Naive approach\n","# Function to convert X_flat and bids_flat into input format for the model\n","def prepare_inputs(X, bids):\n","    inputs = {f'X{i+1}': X[:, i].reshape(-1, 1) for i in range(X.shape[1])}\n","    inputs['z'] = bids.reshape(-1, 1)\n","    return inputs\n","\n","# Prepare the inputs for the neural network\n","inputs_for_nn = prepare_inputs(X_flat, bids_flat)\n","\n","# Step 3: Define the neural network model for the naive approach\n","inputs = {key: tf.keras.layers.Input(shape=(1,), name=key) for key in inputs_for_nn.keys() if key != 'z'}\n","input_num = tf.keras.layers.Concatenate(axis=-1)(list(inputs.values()))\n","input_num = tf.keras.layers.BatchNormalization()(input_num)\n","\n","pCTR = tf.keras.layers.Dense(256, activation=\"swish\")(input_num)\n","pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","pCTR = tf.keras.layers.Dense(256, activation=\"relu\")(pCTR)\n","pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","pCTR = tf.keras.layers.Dense(256, activation=\"relu\")(pCTR)\n","pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","pCTR = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"click\")(pCTR)\n","pCTR_model = tf.keras.Model(inputs=list(inputs.values()), outputs=pCTR)\n","\n","model = tf.keras.Model(\n","    inputs=list(inputs.values()), outputs=pCTR_model.output\n",")\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),\n","    loss=\"binary_crossentropy\",\n",")\n","\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=\"sim/checkpoints/naive_checkpoint\",\n","    save_best_only=True,\n","    verbose=1,\n","    monitor=\"loss\",\n",")\n","\n","model.fit(inputs_for_nn, y_flat, sample_weight=D_flat, epochs=30, steps_per_epoch=1000, callbacks=[cp_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":543346,"status":"ok","timestamp":1716287688283,"user":{"displayName":"R E","userId":"10479407368002365455"},"user_tz":-540},"id":"AXAA5pWqEXLD","outputId":"26333773-4792-42c1-8f37-da10d4fc1766"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1000/1000 [==============================] - ETA: 0s - loss: 0.1891 - click_loss: 4.6045e-04 - impression_iv_loss: 0.1887\n","Epoch 1: click_loss improved from inf to 0.00046, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 17s 14ms/step - loss: 0.1891 - click_loss: 4.6045e-04 - impression_iv_loss: 0.1887\n","Epoch 2/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.1205 - click_loss: 1.9384e-05 - impression_iv_loss: 0.1205\n","Epoch 2: click_loss improved from 0.00046 to 0.00002, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 15s 15ms/step - loss: 0.1205 - click_loss: 1.9365e-05 - impression_iv_loss: 0.1205\n","Epoch 3/40\n"," 996/1000 [============================>.] - ETA: 0s - loss: 0.1163 - click_loss: 1.1420e-05 - impression_iv_loss: 0.1163\n","Epoch 3: click_loss improved from 0.00002 to 0.00001, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1164 - click_loss: 1.1384e-05 - impression_iv_loss: 0.1164\n","Epoch 4/40\n"," 996/1000 [============================>.] - ETA: 0s - loss: 0.1139 - click_loss: 2.8675e-06 - impression_iv_loss: 0.1139\n","Epoch 4: click_loss improved from 0.00001 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1138 - click_loss: 2.8566e-06 - impression_iv_loss: 0.1138\n","Epoch 5/40\n","1000/1000 [==============================] - ETA: 0s - loss: 0.1122 - click_loss: 1.0810e-06 - impression_iv_loss: 0.1122\n","Epoch 5: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1122 - click_loss: 1.0810e-06 - impression_iv_loss: 0.1122\n","Epoch 6/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.1109 - click_loss: 7.3467e-07 - impression_iv_loss: 0.1109\n","Epoch 6: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1109 - click_loss: 7.3401e-07 - impression_iv_loss: 0.1109\n","Epoch 7/40\n"," 995/1000 [============================>.] - ETA: 0s - loss: 0.1100 - click_loss: 2.0212e-07 - impression_iv_loss: 0.1100\n","Epoch 7: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1100 - click_loss: 2.0142e-07 - impression_iv_loss: 0.1100\n","Epoch 8/40\n"," 997/1000 [============================>.] - ETA: 0s - loss: 0.1091 - click_loss: 7.7727e-08 - impression_iv_loss: 0.1091\n","Epoch 8: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1090 - click_loss: 7.7580e-08 - impression_iv_loss: 0.1090\n","Epoch 9/40\n"," 997/1000 [============================>.] - ETA: 0s - loss: 0.1077 - click_loss: 4.1938e-08 - impression_iv_loss: 0.1077\n","Epoch 9: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1078 - click_loss: 4.1870e-08 - impression_iv_loss: 0.1078\n","Epoch 10/40\n"," 997/1000 [============================>.] - ETA: 0s - loss: 0.1077 - click_loss: 1.6614e-08 - impression_iv_loss: 0.1077\n","Epoch 10: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1077 - click_loss: 1.6604e-08 - impression_iv_loss: 0.1077\n","Epoch 11/40\n"," 998/1000 [============================>.] - ETA: 0s - loss: 0.1069 - click_loss: 8.9190e-09 - impression_iv_loss: 0.1069\n","Epoch 11: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1069 - click_loss: 8.9022e-09 - impression_iv_loss: 0.1069\n","Epoch 12/40\n","1000/1000 [==============================] - ETA: 0s - loss: 0.1068 - click_loss: 3.1373e-09 - impression_iv_loss: 0.1068\n","Epoch 12: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1068 - click_loss: 3.1373e-09 - impression_iv_loss: 0.1068\n","Epoch 13/40\n","1000/1000 [==============================] - ETA: 0s - loss: 0.1061 - click_loss: 1.6520e-09 - impression_iv_loss: 0.1061\n","Epoch 13: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1061 - click_loss: 1.6520e-09 - impression_iv_loss: 0.1061\n","Epoch 14/40\n"," 997/1000 [============================>.] - ETA: 0s - loss: 0.1063 - click_loss: 1.0117e-09 - impression_iv_loss: 0.1063\n","Epoch 14: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1063 - click_loss: 1.0095e-09 - impression_iv_loss: 0.1063\n","Epoch 15/40\n"," 996/1000 [============================>.] - ETA: 0s - loss: 0.1052 - click_loss: 5.6416e-10 - impression_iv_loss: 0.1052\n","Epoch 15: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1053 - click_loss: 5.7146e-10 - impression_iv_loss: 0.1053\n","Epoch 16/40\n"," 997/1000 [============================>.] - ETA: 0s - loss: 0.1045 - click_loss: 3.6032e-10 - impression_iv_loss: 0.1045\n","Epoch 16: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 18s 18ms/step - loss: 0.1045 - click_loss: 3.5934e-10 - impression_iv_loss: 0.1045\n","Epoch 17/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.1040 - click_loss: 2.7969e-10 - impression_iv_loss: 0.1040\n","Epoch 17: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 15s 15ms/step - loss: 0.1040 - click_loss: 2.7973e-10 - impression_iv_loss: 0.1040\n","Epoch 18/40\n"," 997/1000 [============================>.] - ETA: 0s - loss: 0.1042 - click_loss: 2.1545e-10 - impression_iv_loss: 0.1042\n","Epoch 18: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 15s 15ms/step - loss: 0.1043 - click_loss: 2.1511e-10 - impression_iv_loss: 0.1043\n","Epoch 19/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.1041 - click_loss: 3.3728e-10 - impression_iv_loss: 0.1041\n","Epoch 19: click_loss did not improve from 0.00000\n","1000/1000 [==============================] - 12s 12ms/step - loss: 0.1041 - click_loss: 3.3700e-10 - impression_iv_loss: 0.1041\n","Epoch 20/40\n"," 996/1000 [============================>.] - ETA: 0s - loss: 0.1034 - click_loss: 1.7042e-10 - impression_iv_loss: 0.1034\n","Epoch 20: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 15s 15ms/step - loss: 0.1035 - click_loss: 1.7011e-10 - impression_iv_loss: 0.1035\n","Epoch 21/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.1036 - click_loss: 1.6334e-10 - impression_iv_loss: 0.1036\n","Epoch 21: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 15s 15ms/step - loss: 0.1036 - click_loss: 1.6424e-10 - impression_iv_loss: 0.1036\n","Epoch 22/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.1032 - click_loss: 4.1183e-10 - impression_iv_loss: 0.1032\n","Epoch 22: click_loss did not improve from 0.00000\n","1000/1000 [==============================] - 12s 12ms/step - loss: 0.1032 - click_loss: 4.1144e-10 - impression_iv_loss: 0.1032\n","Epoch 23/40\n"," 997/1000 [============================>.] - ETA: 0s - loss: 0.1030 - click_loss: 1.3654e-10 - impression_iv_loss: 0.1030\n","Epoch 23: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 15s 15ms/step - loss: 0.1030 - click_loss: 1.3616e-10 - impression_iv_loss: 0.1030\n","Epoch 24/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.1028 - click_loss: 1.3419e-10 - impression_iv_loss: 0.1028\n","Epoch 24: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 15s 15ms/step - loss: 0.1028 - click_loss: 1.3407e-10 - impression_iv_loss: 0.1028\n","Epoch 25/40\n","1000/1000 [==============================] - ETA: 0s - loss: 0.1024 - click_loss: 1.2133e-10 - impression_iv_loss: 0.1024\n","Epoch 25: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1024 - click_loss: 1.2133e-10 - impression_iv_loss: 0.1024\n","Epoch 26/40\n"," 997/1000 [============================>.] - ETA: 0s - loss: 0.1018 - click_loss: 1.0554e-10 - impression_iv_loss: 0.1018\n","Epoch 26: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1020 - click_loss: 1.0551e-10 - impression_iv_loss: 0.1020\n","Epoch 27/40\n"," 998/1000 [============================>.] - ETA: 0s - loss: 0.1016 - click_loss: 1.0779e-10 - impression_iv_loss: 0.1016\n","Epoch 27: click_loss did not improve from 0.00000\n","1000/1000 [==============================] - 11s 11ms/step - loss: 0.1016 - click_loss: 1.0761e-10 - impression_iv_loss: 0.1016\n","Epoch 28/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.1020 - click_loss: 8.6510e-11 - impression_iv_loss: 0.1020\n","Epoch 28: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1020 - click_loss: 8.6426e-11 - impression_iv_loss: 0.1020\n","Epoch 29/40\n"," 998/1000 [============================>.] - ETA: 0s - loss: 0.1012 - click_loss: 7.4654e-11 - impression_iv_loss: 0.1012\n","Epoch 29: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1012 - click_loss: 7.4514e-11 - impression_iv_loss: 0.1012\n","Epoch 30/40\n","1000/1000 [==============================] - ETA: 0s - loss: 0.1011 - click_loss: 9.1012e-11 - impression_iv_loss: 0.1011\n","Epoch 30: click_loss did not improve from 0.00000\n","1000/1000 [==============================] - 11s 11ms/step - loss: 0.1011 - click_loss: 9.1012e-11 - impression_iv_loss: 0.1011\n","Epoch 31/40\n"," 996/1000 [============================>.] - ETA: 0s - loss: 0.1008 - click_loss: 8.9849e-11 - impression_iv_loss: 0.1008\n","Epoch 31: click_loss did not improve from 0.00000\n","1000/1000 [==============================] - 11s 11ms/step - loss: 0.1009 - click_loss: 8.9608e-11 - impression_iv_loss: 0.1009\n","Epoch 32/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.1006 - click_loss: 7.0609e-11 - impression_iv_loss: 0.1006\n","Epoch 32: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.1006 - click_loss: 7.0590e-11 - impression_iv_loss: 0.1006\n","Epoch 33/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.1003 - click_loss: 8.2598e-11 - impression_iv_loss: 0.1003\n","Epoch 33: click_loss did not improve from 0.00000\n","1000/1000 [==============================] - 11s 11ms/step - loss: 0.1003 - click_loss: 8.2541e-11 - impression_iv_loss: 0.1003\n","Epoch 34/40\n"," 998/1000 [============================>.] - ETA: 0s - loss: 0.1002 - click_loss: 1.0174e-10 - impression_iv_loss: 0.1002\n","Epoch 34: click_loss did not improve from 0.00000\n","1000/1000 [==============================] - 10s 10ms/step - loss: 0.1002 - click_loss: 1.0157e-10 - impression_iv_loss: 0.1002\n","Epoch 35/40\n"," 997/1000 [============================>.] - ETA: 0s - loss: 0.1006 - click_loss: 7.3924e-11 - impression_iv_loss: 0.1006\n","Epoch 35: click_loss did not improve from 0.00000\n","1000/1000 [==============================] - 11s 11ms/step - loss: 0.1005 - click_loss: 7.3732e-11 - impression_iv_loss: 0.1005\n","Epoch 36/40\n","1000/1000 [==============================] - ETA: 0s - loss: 0.0997 - click_loss: 8.0427e-11 - impression_iv_loss: 0.0997\n","Epoch 36: click_loss did not improve from 0.00000\n","1000/1000 [==============================] - 11s 11ms/step - loss: 0.0997 - click_loss: 8.0427e-11 - impression_iv_loss: 0.0997\n","Epoch 37/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.0999 - click_loss: 6.6244e-11 - impression_iv_loss: 0.0999\n","Epoch 37: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.0999 - click_loss: 6.6268e-11 - impression_iv_loss: 0.0999\n","Epoch 38/40\n"," 998/1000 [============================>.] - ETA: 0s - loss: 0.0996 - click_loss: 5.7914e-11 - impression_iv_loss: 0.0996\n","Epoch 38: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.0996 - click_loss: 5.7820e-11 - impression_iv_loss: 0.0996\n","Epoch 39/40\n"," 997/1000 [============================>.] - ETA: 0s - loss: 0.0998 - click_loss: 4.8096e-11 - impression_iv_loss: 0.0998\n","Epoch 39: click_loss improved from 0.00000 to 0.00000, saving model to sim/checkpoints/baselineIV_checkpoint\n","1000/1000 [==============================] - 14s 14ms/step - loss: 0.0998 - click_loss: 4.7964e-11 - impression_iv_loss: 0.0998\n","Epoch 40/40\n"," 999/1000 [============================>.] - ETA: 0s - loss: 0.0993 - click_loss: 7.5627e-11 - impression_iv_loss: 0.0993\n","Epoch 40: click_loss did not improve from 0.00000\n","1000/1000 [==============================] - 11s 11ms/step - loss: 0.0993 - click_loss: 7.5618e-11 - impression_iv_loss: 0.0993\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7870652a33d0>"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["### bs-iv\n","# Function to convert X_flat and bids_flat into input format for the model\n","def prepare_inputs(X, bids):\n","    inputs = {f'X{i+1}': X[:, i].reshape(-1, 1) for i in range(X.shape[1])}\n","    inputs['z'] = bids.reshape(-1, 1)\n","    return inputs\n","\n","# Prepare the inputs for the neural network\n","inputs_for_nn = prepare_inputs(X_flat, bids_flat)\n","\n","# Step 3: Define the neural network model\n","inputs = {key: tf.keras.layers.Input(shape=(1,), name=key) for key in inputs_for_nn.keys() if key != 'z'}\n","inputs_iv = {\"z\": tf.keras.layers.Input(shape=(1,), name=\"z\")}\n","inputs_with_iv = {**inputs, **inputs_iv}\n","\n","inputs_net = tf.keras.layers.Concatenate(axis=-1)(list(inputs.values()))\n","inputs_iv_net = inputs_iv['z']\n","inputs_net_iv = tf.keras.layers.Concatenate(axis=-1)([inputs_net, inputs_iv_net])\n","\n","pIMP_iv = tf.keras.layers.Dense(128, activation=\"swish\")(inputs_net_iv)\n","pIMP_iv = tf.keras.layers.BatchNormalization()(pIMP_iv)\n","pIMP_iv = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"impression_iv\")(pIMP_iv)\n","pIMP_iv_model = tf.keras.Model(inputs=inputs_with_iv, outputs=pIMP_iv)\n","\n","input_net = tf.keras.layers.Concatenate(axis=-1)([inputs_net, pIMP_iv_model.output])\n","pCTR = tf.keras.layers.Dense(256, activation=\"swish\")(input_net)\n","pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","pCTR = tf.keras.layers.Dense(256, activation=\"relu\")(pCTR)\n","pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","pCTR = tf.keras.layers.Dense(256, activation=\"relu\")(pCTR)\n","pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","pCTR = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"click\")(pCTR)\n","pCTR_model = tf.keras.Model(inputs=inputs_with_iv, outputs=pCTR)\n","\n","model = tf.keras.Model(\n","    inputs=inputs_with_iv, outputs=[pCTR_model.output, pIMP_iv_model.output]\n",")\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),\n","    loss=\"binary_crossentropy\",\n",")\n","\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=\"sim/checkpoints/baselineIV_checkpoint\",\n","    save_best_only=True,\n","    verbose=1,\n","    monitor=\"click_loss\",\n",")\n","imp_weight = np.ones_like(D_flat)\n","model.fit({**inputs_for_nn}, [y_flat, D_flat], sample_weight=[D_flat, imp_weight], epochs=40, steps_per_epoch=1000, callbacks=[cp_callback])"]},{"cell_type":"markdown","metadata":{"id":"wcpZDp3Safyj"},"source":["### Step 1 & 2: Initializing and generating historical data"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":412,"status":"ok","timestamp":1718109160429,"user":{"displayName":"R E","userId":"10479407368002365455"},"user_tz":-540},"id":"Vv5WHGixk5zB"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from scipy.special import logit, expit\n","from scipy.stats import bernoulli, multivariate_normal\n","from sklearn.linear_model import LogisticRegression\n","\n","class AuctionData:\n","    def __init__(self):\n","        self.alpha = None\n","        self.beta = None\n","        self.gamma = None\n","        self.pCTR_biased = None\n","\n","    def generate_parameters(self, num_features):\n","        self.alpha = np.random.normal(0.1, 1, num_features)\n","        self.beta = np.random.normal(0.1, 1, num_features)\n","        self.gamma = np.random.normal(0.1, 1, num_features)\n","        #A = np.random.normal(0, 1, (num_features + 1, num_features + 1))\n","        #self.Sigma = np.dot(A, A.T)\n","\n","    def sample_reparameterized_beta(self, mu, phi, size):\n","        mu = np.clip(mu, 0.01, 0.99)\n","        alpha = mu * phi\n","        beta_param = (1 - mu) * phi\n","        return np.random.beta(alpha, beta_param, size=size)\n","\n","    def initialize_training_data(self, num_records_initial, num_features):\n","        X_k = []\n","        eta_k = []\n","        y_k = []\n","        D_k = []\n","        \n","        while len(D_k) < 5000:\n","            X_binary = np.random.binomial(1, 0.5, (num_records_initial, 10))\n","            X_uniform_5 = np.random.uniform(-5, 5, (num_records_initial, 10))\n","            X_uniform_2 = np.random.uniform(-2, 2, (num_records_initial, 5))\n","            X_tmp = np.hstack((X_binary, X_uniform_5, X_uniform_2))\n","            eta_tmp = np.random.uniform(-5, 5, num_records_initial)\n","            \n","            D_tmp_prob = expit(np.dot(X_tmp, self.gamma) + eta_tmp)\n","            D_tmp = bernoulli.rvs(D_tmp_prob)\n","            \n","            X_tmp = X_tmp[D_tmp == 1]\n","            eta_tmp = eta_tmp[D_tmp == 1]\n","            p_tmp = expit(np.dot(X_tmp, self.beta) + eta_tmp)\n","            y_tmp = bernoulli.rvs(p_tmp)\n","            D_tmp = D_tmp[D_tmp == 1]\n","            \n","            X_k.extend(X_tmp)\n","            eta_k.extend(eta_tmp)\n","            y_k.extend(y_tmp)\n","            D_k.extend(D_tmp)\n","\n","        return np.array(X_k)[:5000], np.array(eta_k)[:5000], np.array(y_k)[:5000], np.array(D_k)[:5000]\n","\n","    def train_logistic_regression(self, X_k, y_k):\n","        self.pCTR_biased = LogisticRegression()\n","        self.pCTR_biased.fit(X_k, y_k)\n","\n","    def generate_historical_auction_data(self, num_auctions, num_auctioneers_per_auction, num_features):\n","        historical_data = []\n","        for _ in range(num_auctions):\n","            X_binary = np.random.binomial(1, 0.5, (num_auctioneers_per_auction, 10))\n","            X_uniform_5 = np.random.uniform(-5, 5, (num_auctioneers_per_auction, 10))\n","            X_uniform_2 = np.random.uniform(-2, 2, (num_auctioneers_per_auction, 5))\n","            X_j = np.hstack((X_binary, X_uniform_5, X_uniform_2))\n","\n","            eta_j = np.random.uniform(-5, 5, num_auctioneers_per_auction)#np.random.normal(0, 1, num_auctioneers_per_auction)\n","            mu_j = expit(np.dot(X_j, self.alpha))\n","            bids = self.sample_reparameterized_beta(mu_j, 2, size=num_auctioneers_per_auction)\n","            pCTR_j = self.pCTR_biased.predict_proba(X_j)[:, 1]\n","            auction_scores = bids * pCTR_j\n","            j_star = np.argmax(auction_scores)\n","            p_j = expit(np.dot(X_j[j_star], self.beta) + eta_j[j_star])\n","            y_j = np.zeros(num_auctioneers_per_auction)\n","            y_j[j_star] = bernoulli.rvs(p_j)\n","            D_j = np.zeros(num_auctioneers_per_auction)\n","            D_j[j_star] = 1\n","            historical_data.append((y_j, X_j, bids, D_j))\n","\n","        return historical_data\n","\n","    def flatten_historical_data(self, historical_data):\n","        y_flat = []\n","        X_flat = []\n","        bids_flat = []\n","        D_flat = []\n","\n","        for y_j, X_j, bids, D_j in historical_data:\n","            y_flat.extend(y_j)\n","            X_flat.extend(X_j)\n","            bids_flat.extend(bids)\n","            D_flat.extend(D_j)\n","\n","        y_flat = np.array(y_flat)\n","        X_flat = np.array(X_flat)\n","        bids_flat = np.array(bids_flat)\n","        D_flat = np.array(D_flat)\n","\n","        return y_flat, X_flat, bids_flat, D_flat\n","\n","    def generate_and_train(self, num_records_initial=5000, num_auctions=5000, num_auctioneers_per_auction=20, num_features=25, seed=None):\n","        # Set the seed for this replication\n","        if seed is not None:\n","            np.random.seed(seed)\n","            tf.random.set_seed(seed)\n","        \n","        # Generate parameters\n","        self.generate_parameters(num_features)\n","\n","        # Step 1: Initializing and generating training data\n","        X_k, eta_k, y_k, D_k = self.initialize_training_data(num_records_initial, num_features)\n","\n","        # Define a logistic regression model for pCTR and train it\n","        self.train_logistic_regression(X_k, y_k)\n","\n","        # Step 2: Generating historical auction data\n","        historical_data = self.generate_historical_auction_data(num_auctions, num_auctioneers_per_auction, num_features)\n","\n","        # Flatten the historical data for learning\n","        y_flat, X_flat, bids_flat, D_flat = self.flatten_historical_data(historical_data)\n","\n","        return y_flat, X_flat, bids_flat, D_flat, self.alpha, self.beta, self.gamma\n","\n","# Usage example\n","#auction_data = AuctionData()\n","#y_flat, X_flat, bids_flat, D_flat, alpha, beta, gamma = auction_data.generate_and_train()"]},{"cell_type":"markdown","metadata":{"id":"UMBcSv-xMlWV"},"source":["### Step 3: Define and learn the baselines"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8728,"status":"ok","timestamp":1718089842521,"user":{"displayName":"R E","userId":"10479407368002365455"},"user_tz":-540},"id":"gY3GSjhcg8Wt"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/emo_157/Desktop/24_coldstart/sim/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n","  warnings.warn(\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","\n","class Naive:\n","    def __init__(self):\n","        self.model = None\n","\n","    def prepare_inputs(self, X, bids):\n","        inputs = {f'X{i+1}': X[:, i].reshape(-1, 1) for i in range(X.shape[1])}\n","        inputs['z'] = bids.reshape(-1, 1)\n","        return inputs\n","\n","    def create_model(self, input_shape):\n","        inputs = {f'X{i+1}': tf.keras.layers.Input(shape=(1,), name=f'X{i+1}') for i in range(input_shape[1])}\n","        inputs_iv = {\"z\": tf.keras.layers.Input(shape=(1,), name=\"z\")}\n","        inputs_with_iv = {**inputs, **inputs_iv}\n","\n","        input_net = tf.keras.layers.Concatenate(axis=-1)(list(inputs.values()))\n","        input_net = tf.keras.layers.BatchNormalization()(input_net)\n","        input_net_iv = tf.keras.layers.Concatenate(axis=-1)([input_net, inputs_iv['z']])\n","\n","        pCTR = tf.keras.layers.Dense(256, activation=\"swish\")(input_net_iv)\n","        pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","        pCTR = tf.keras.layers.Dense(256, activation=\"relu\")(pCTR)\n","        pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","        pCTR = tf.keras.layers.Dense(256, activation=\"relu\")(pCTR)\n","        pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","        pCTR = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"click\")(pCTR)\n","        model = tf.keras.Model(inputs=list(inputs_with_iv.values()), outputs=pCTR)\n","\n","        model.compile(\n","            optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),\n","            loss=\"binary_crossentropy\",\n","        )\n","        return model\n","\n","    def train_model(self, X_flat, bids_flat, y_flat, D_flat, epochs=40, steps_per_epoch=1000):\n","        inputs_for_nn = self.prepare_inputs(X_flat, bids_flat)\n","        self.model = self.create_model(X_flat.shape)\n","\n","        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","            filepath=\"checkpoints/naive_checkpoint\",\n","            save_best_only=True,\n","            verbose=1,\n","            monitor=\"loss\",\n","        )\n","        es_callback = tf.keras.callbacks.EarlyStopping(\n","            monitor='loss',\n","            patience=3,\n","            verbose=1\n","        )\n","\n","        self.model.fit(\n","            {**inputs_for_nn},\n","            y_flat,\n","            sample_weight=D_flat,\n","            epochs=epochs,\n","            steps_per_epoch=steps_per_epoch,\n","            callbacks=[cp_callback, es_callback]\n","        )\n","        return self.model\n","\n","# Usage example\n","#naive_instance = Naive()\n","#naive_model = naive_instance.train_model(X_flat, bids_flat, y_flat, D_flat)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1718089842521,"user":{"displayName":"R E","userId":"10479407368002365455"},"user_tz":-540},"id":"ZAT7r2dDmbX6"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","class IVBS:\n","    def __init__(self):\n","        self.model = None\n","\n","    def prepare_inputs(self, X, bids):\n","        inputs = {f'X{i+1}': X[:, i].reshape(-1, 1) for i in range(X.shape[1])}\n","        inputs['z'] = bids.reshape(-1, 1)\n","        return inputs\n","\n","    def create_model(self, input_shape):\n","        inputs = {f'X{i+1}': tf.keras.layers.Input(shape=(1,), name=f'X{i+1}') for i in range(input_shape[1])}\n","        inputs_iv = {\"z\": tf.keras.layers.Input(shape=(1,), name=\"z\")}\n","        inputs_with_iv = {**inputs, **inputs_iv}\n","\n","        inputs_net = tf.keras.layers.Concatenate(axis=-1)(list(inputs.values()))\n","        inputs_iv_net = list(inputs_iv.values())[0]\n","        inputs_net_iv = tf.keras.layers.Concatenate(axis=-1)([inputs_net, inputs_iv_net])\n","\n","        pIMP_iv = tf.keras.layers.Dense(128, activation=\"swish\")(inputs_net_iv)\n","        pIMP_iv = tf.keras.layers.BatchNormalization()(pIMP_iv)\n","        pIMP_iv = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"impression_iv\")(pIMP_iv)\n","        pIMP_iv_model = tf.keras.Model(inputs=inputs_with_iv, outputs=pIMP_iv)\n","\n","        model = tf.keras.Model(inputs=inputs_with_iv, outputs=pIMP_iv_model.output)\n","        model.compile(\n","            optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),\n","            loss=\"binary_crossentropy\",\n","        )\n","        return model\n","\n","    def train_model(self, X_flat, bids_flat, y_flat, D_flat, epochs=40, steps_per_epoch=1000):\n","        inputs_for_nn = self.prepare_inputs(X_flat, bids_flat)\n","        self.model = self.create_model(X_flat.shape)\n","\n","        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","            filepath=\"checkpoints/IV-BS_checkpoint\",\n","            save_best_only=True,\n","            verbose=1,\n","            monitor=\"loss\",\n","        )\n","        es_callback = tf.keras.callbacks.EarlyStopping(\n","            monitor='loss',\n","            patience=3,\n","            verbose=1\n","        )\n","\n","        self.model.fit(\n","            {**inputs_for_nn},\n","            y_flat,\n","            sample_weight=D_flat,\n","            epochs=epochs,\n","            steps_per_epoch=steps_per_epoch,\n","            callbacks=[cp_callback, es_callback]\n","        )\n","        return self.model\n","\n","# Usage example\n","#IV-BS_instance = IV-BS()\n","#ivbs_model = ivbs_instance.train_model(X_flat, bids_flat, y_flat, D_flat)"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"id":"wLx-479JEYVm"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","class IPS:\n","    def __init__(self):\n","        self.pIMP_model = None\n","        self.pCTR_model = None\n","\n","    def prepare_inputs(self, X, bids):\n","        inputs = {f'X{i+1}': X[:, i].reshape(-1, 1) for i in range(X.shape[1])}\n","        inputs['z'] = bids.reshape(-1, 1)\n","        return inputs\n","\n","    def create_pIMP_model(self, input_shape):\n","        inputs = {f'X{i+1}': tf.keras.layers.Input(shape=(1,), name=f'X{i+1}') for i in range(input_shape[1])}\n","        inputs_iv = {\"z\": tf.keras.layers.Input(shape=(1,), name=\"z\")}\n","        inputs_with_iv = {**inputs, **inputs_iv}\n","\n","        inputs_net = tf.keras.layers.Concatenate(axis=-1)(list(inputs.values()))\n","        inputs_iv_net = list(inputs_iv.values())[0]\n","        inputs_net_iv = tf.keras.layers.Concatenate(axis=-1)([inputs_net, inputs_iv_net])\n","\n","        pIMP_iv = tf.keras.layers.Dense(128, activation=\"swish\")(inputs_net_iv)\n","        pIMP_iv = tf.keras.layers.BatchNormalization()(pIMP_iv)\n","        pIMP_iv = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"impression_iv\")(pIMP_iv)\n","        pIMP_iv_model = tf.keras.Model(inputs=inputs_with_iv, outputs=pIMP_iv)\n","\n","        pIMP_iv_model.compile(\n","            optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),\n","            loss=\"binary_crossentropy\",\n","        )\n","\n","        return pIMP_iv_model\n","\n","    def train_pIMP_model(self, X_flat, bids_flat, y_flat, epochs=50, steps_per_epoch=1000):\n","        inputs_for_nn = self.prepare_inputs(X_flat, bids_flat)\n","        self.pIMP_model = self.create_pIMP_model(X_flat.shape)\n","\n","        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","            filepath=\"checkpoints/ipsimp_checkpoint\",\n","            save_best_only=True,\n","            verbose=1,\n","            monitor=\"loss\",\n","        )\n","        es_callback = tf.keras.callbacks.EarlyStopping(\n","            monitor='loss',\n","            patience=3,\n","            verbose=1\n","        )\n","\n","        self.pIMP_model.fit(inputs_for_nn, y_flat, epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=[cp_callback, es_callback])\n","        return self.pIMP_model\n","\n","    def create_pCTR_model(self, input_shape):\n","        inputs = {f'X{i+1}': tf.keras.layers.Input(shape=(1,), name=f'X{i+1}') for i in range(input_shape[1])}\n","        inputs_net = tf.keras.layers.Concatenate(axis=-1)(list(inputs.values()))\n","\n","        pCTR = tf.keras.layers.Dense(256, activation=\"swish\")(inputs_net)\n","        pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","        pCTR = tf.keras.layers.Dense(256, activation=\"relu\")(pCTR)\n","        pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","        pCTR = tf.keras.layers.Dense(256, activation=\"relu\")(pCTR)\n","        pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","        pCTR = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"click\")(pCTR)\n","        pCTR_model = tf.keras.Model(inputs=inputs, outputs=pCTR)\n","\n","        pCTR_model.compile(\n","            optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),\n","            loss=\"binary_crossentropy\"\n","        )\n","\n","        return pCTR_model\n","\n","    def train_pCTR_model(self, X_flat, bids_flat, y_flat, epochs=10, steps_per_epoch=1000):\n","        inputs_for_nn = self.prepare_inputs(X_flat, bids_flat)\n","        self.pCTR_model = self.create_pCTR_model(X_flat.shape)\n","\n","        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","            filepath=\"checkpoints/ipsclk_checkpoint\",\n","            save_best_only=True,\n","            verbose=1,\n","            monitor=\"loss\",\n","        )\n","        es_callback = tf.keras.callbacks.EarlyStopping(\n","            monitor='loss',\n","            patience=3,\n","            verbose=1\n","        )\n","\n","        # Load the pIMP model and predict the impression probabilities\n","        ipsimp = tf.keras.models.load_model(\"checkpoints/ipsimp_checkpoint\")\n","        ips_weight = ipsimp.predict(inputs_for_nn, batch_size=100000, verbose=1).flatten()\n","\n","        # Train the pCTR model with the impression weights\n","        self.pCTR_model.fit(inputs_for_nn, y_flat, sample_weight=ips_weight, epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=[cp_callback, es_callback])\n","        return self.pCTR_model\n","\n","# Usage example\n","#ips_instance = IPS()\n","#ips_instance.train_pIMP_model(X_flat, bids_flat, y_flat)\n","#ips_model = ips_instance.train_pCTR_model(X_flat, bids_flat, y_flat)"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":3379,"status":"ok","timestamp":1718110392108,"user":{"displayName":"R E","userId":"10479407368002365455"},"user_tz":-540},"id":"AoJADhIqoP7m"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","class UBIPS:\n","    def __init__(self):\n","        self.model = None\n","\n","    def prepare_inputs(self, X, bids):\n","        inputs = {f'X{i+1}': X[:, i].reshape(-1, 1) for i in range(X.shape[1])}\n","        inputs['z'] = bids.reshape(-1, 1)\n","        return inputs\n","\n","    def create_model(self, input_shape):\n","        inputs = {f'X{i+1}': tf.keras.layers.Input(shape=(1,), name=f'X{i+1}') for i in range(input_shape[1])}\n","        inputs_iv = {\"z\": tf.keras.layers.Input(shape=(1,), name=\"z\")}\n","        inputs_with_iv = {**inputs, **inputs_iv}\n","\n","        input_net = tf.keras.layers.Concatenate(axis=-1)(list(inputs.values()))\n","        input_net = tf.keras.layers.BatchNormalization()(input_net)\n","        input_net_iv = tf.keras.layers.Concatenate(axis=-1)([input_net, inputs_iv['z']])\n","\n","        pIMP_iv = tf.keras.layers.Dense(128, activation=\"swish\")(input_net_iv)\n","        pIMP_iv = tf.keras.layers.BatchNormalization()(pIMP_iv)\n","        pIMP_iv = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"impression_iv\")(pIMP_iv)\n","        pIMP_iv_model = tf.keras.Model(inputs=inputs_with_iv, outputs=pIMP_iv)\n","\n","        pCTR = tf.keras.layers.Dense(256, activation=\"swish\")(input_net)\n","        pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","        pCTR = tf.keras.layers.Dense(256, activation=\"relu\")(pCTR)\n","        pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","        pCTR = tf.keras.layers.Dense(256, activation=\"relu\")(pCTR)\n","        pCTR = tf.keras.layers.BatchNormalization()(pCTR)\n","        pCTR = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"click\")(pCTR)\n","        pCTR_model = tf.keras.Model(inputs=inputs_with_iv, outputs=pCTR)\n","\n","        model = tf.keras.Model(\n","            inputs=inputs_with_iv,\n","            outputs=[pCTR_model.output * pIMP_iv_model.output]\n","        )\n","        model.compile(\n","            optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),\n","            loss=\"binary_crossentropy\",\n","        )\n","\n","        return model\n","\n","    def train_model(self, X_flat, bids_flat, y_flat, D_flat, epochs=50, steps_per_epoch=1000):\n","        inputs_for_nn = self.prepare_inputs(X_flat, bids_flat)\n","        self.model = self.create_model(X_flat.shape)\n","\n","        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","            filepath=\"checkpoints/ubips_checkpoint\",\n","            save_best_only=True,\n","            verbose=1,\n","            monitor=\"loss\",\n","        )\n","        es_callback = tf.keras.callbacks.EarlyStopping(\n","            monitor='loss',\n","            patience=7,\n","            verbose=1\n","        )\n","\n","        self.model.fit(\n","            {**inputs_for_nn},\n","            [y_flat],\n","            epochs=epochs,\n","            steps_per_epoch=steps_per_epoch,\n","            callbacks=[cp_callback, es_callback]\n","        )\n","        return self.model\n","\n","# Usage example\n","#ubips_instance = UBIPS()\n","#ubips_model = ubips_instance.train_model(X_flat, bids_flat, y_flat, D_flat)"]},{"cell_type":"markdown","metadata":{"id":"bIL7u_ZHYG9S"},"source":["### Step 4: Validating baselines with independently displayed data"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/qb/31p7jmc96_n0fns67992g7x00000gn/T/ipykernel_17521/1861601639.py:132: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  plt.boxplot(data[metric], labels=models, patch_artist=True,\n","/var/folders/qb/31p7jmc96_n0fns67992g7x00000gn/T/ipykernel_17521/1861601639.py:132: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  plt.boxplot(data[metric], labels=models, patch_artist=True,\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from sklearn.metrics import log_loss, roc_auc_score\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from scipy.special import expit\n","from scipy.stats import bernoulli\n","import os\n","\n","class ModelEvaluation:\n","    def __init__(self, replication_times=20, num_auctions=5000):\n","        self.replication_times = replication_times\n","        self.num_auctions = num_auctions\n","        self.results = {\n","            'Naive': {'LogLoss': [], 'AUC': []},\n","            'IV-BS': {'LogLoss': [], 'AUC': []},\n","            'UBIPS': {'LogLoss': [], 'AUC': []}\n","        }\n","        self.checkpoints_dir = \"bs_checkpoints\"\n","        self.quantile_results = {\n","            'Naive': {'LogLoss': [], 'AUC': []},\n","            'IV-BS': {'LogLoss': [], 'AUC': []},\n","            'UBIPS': {'LogLoss': [], 'AUC': []}\n","        }\n","\n","        os.makedirs(self.checkpoints_dir, exist_ok=True)\n","        os.makedirs(\"res\", exist_ok=True)\n","        os.makedirs(\"figs\", exist_ok=True)\n","\n","    def evaluate_model(self, auction_data):\n","        for iteration in tqdm(range(self.replication_times)):\n","            # Generate training data and train models\n","            y_flat, X_flat, bids_flat, D_flat, alpha, beta, gamma = auction_data.generate_and_train()\n","\n","            naive = Naive()\n","            naive_model = naive.train_model(X_flat, bids_flat, y_flat, D_flat)\n","            naive_model.save(f\"{self.checkpoints_dir}/naive_model_{iteration}.h5\")\n","\n","            ivbs = IVBS()\n","            ivbs_model = ivbs.train_model(X_flat, bids_flat, y_flat, D_flat)\n","            ivbs_model.save(f\"{self.checkpoints_dir}/ivbs_model_{iteration}.h5\")\n","\n","            ubips = UBIPS()\n","            ubips_model = ubips.train_model(X_flat, bids_flat, y_flat, D_flat)\n","            ubips_model.save(f\"{self.checkpoints_dir}/ubips_model_{iteration}.h5\")\n","\n","            # Generate validation data\n","            num_records_validation = 50000\n","            X_binary = np.random.binomial(1, 0.5, (num_records_validation, 10))\n","            X_uniform_5 = np.random.uniform(-5, 5, (num_records_validation, 10))\n","            X_uniform_2 = np.random.uniform(-2, 2, (num_records_validation, 5))\n","            X_l = np.hstack((X_binary, X_uniform_5, X_uniform_2))\n","            eta_l = np.random.uniform(-5, 5, num_records_validation)\n","            mu_l = expit(np.dot(X_l, alpha))\n","            bids = auction_data.sample_reparameterized_beta(mu_l, 2, size=num_records_validation)\n","            p_l = expit(np.dot(X_l, beta) + eta_l)\n","            y_l = bernoulli.rvs(p_l)\n","            inputs_for_validation = naive.prepare_inputs(X_l, bids)\n","\n","            # Save validation data\n","            np.savez(f\"{self.checkpoints_dir}/validation_data_{iteration}.npz\", X_l=X_l, bids=bids, y_l=y_l)\n","\n","            # Load the models\n","            naive = tf.keras.models.load_model(f\"{self.checkpoints_dir}/naive_model_{iteration}.h5\")\n","            ivbs = tf.keras.models.load_model(f\"{self.checkpoints_dir}/ivbs_model_{iteration}.h5\")\n","            ubips = tf.keras.models.load_model(f\"{self.checkpoints_dir}/ubips_model_{iteration}.h5\")\n","\n","            # Naive model prediction\n","            naive_predictions = naive.predict(inputs_for_validation).flatten()\n","            naive_logloss = log_loss(y_l, naive_predictions)\n","            naive_auc = roc_auc_score(y_l, naive_predictions)\n","\n","            # IV-BS model prediction\n","            ivbs_predictions = ivbs.predict(inputs_for_validation)\n","            if isinstance(ivbs_predictions, list):\n","                ivbs_predictions = ivbs_predictions[0].flatten()\n","            else:\n","                ivbs_predictions = ivbs_predictions.flatten()\n","            ivbs_logloss = log_loss(y_l, ivbs_predictions)\n","            ivbs_auc = roc_auc_score(y_l, ivbs_predictions)\n","\n","            # UBIPS model prediction\n","            ubips_predictions = ubips.predict(inputs_for_validation)\n","            if isinstance(ubips_predictions, list):\n","                ubips_predictions = ubips_predictions[0].flatten()\n","            else:\n","                ubips_predictions = ubips_predictions.flatten()\n","            ubips_logloss = log_loss(y_l, ubips_predictions)\n","            ubips_auc = roc_auc_score(y_l, ubips_predictions)\n","\n","            # Store results\n","            self.results['Naive']['LogLoss'].append(naive_logloss)\n","            self.results['Naive']['AUC'].append(naive_auc)\n","\n","            self.results['IV-BS']['LogLoss'].append(ivbs_logloss)\n","            self.results['IV-BS']['AUC'].append(ivbs_auc)\n","\n","            self.results['UBIPS']['LogLoss'].append(ubips_logloss)\n","            self.results['UBIPS']['AUC'].append(ubips_auc)\n","\n","            # Quantile validation\n","            filtered_dfs = self.create_filtered_dfs(X_l, bids, eta_l, y_l)\n","            quantile_results = self.evaluate_quantiles(filtered_dfs, naive, ivbs, ubips)\n","\n","            for model in self.results.keys():\n","                for metric in ['LogLoss', 'AUC']:\n","                    self.quantile_results[model][metric].extend(quantile_results[model][metric])\n","\n","            self.save_results()\n","\n","    def save_results(self):\n","        np.save(\"res/results.npy\", self.results)\n","        np.save(\"res/quantile_results.npy\", self.quantile_results)\n","\n","    def load_results(self):\n","        self.results = np.load(\"res/results.npy\", allow_pickle=True).item()\n","        self.quantile_results = np.load(\"res/quantile_results.npy\", allow_pickle=True).item()\n","\n","    def plot_results(self):\n","        models = ['Naive', 'IV-BS', 'UBIPS']\n","        metrics = ['AUC', 'LogLoss']\n","        colors = ['#1f77b4', '#ff7f0e', '#8c564b']\n","\n","        data = {metric: [] for metric in metrics}\n","        for metric in metrics:\n","            for model in models:\n","                data[metric].append(self.results[model][metric])\n","\n","        plt.figure(figsize=(15, 10))\n","        for i, metric in enumerate(metrics):\n","            plt.subplot(1, 2, i + 1)\n","            plt.boxplot(data[metric], labels=models, patch_artist=True,\n","                        boxprops=dict(facecolor=colors[i], color=colors[i]),\n","                        medianprops=dict(color='black'),\n","                        whiskerprops=dict(color=colors[i]),\n","                        capprops=dict(color=colors[i]),\n","                        flierprops=dict(markerfacecolor=colors[i], markeredgecolor=colors[i]))\n","            plt.title(metric)\n","            plt.xlabel('Model')\n","            #plt.ylabel(metric)\n","        plt.savefig(\"figs/results_boxplot.pdf\")\n","        plt.close()\n","\n","    def evaluate_saved_models(self):\n","        models = ['Naive', 'IV-BS', 'UBIPS']\n","        for iteration in range(self.replication_times):\n","            # Load validation data\n","            data = np.load(f\"{self.checkpoints_dir}/validation_data_{iteration}.npz\")\n","            X_l = data['X_l']\n","            bids = data['bids']\n","            y_l = data['y_l']\n","            inputs_for_validation = Naive().prepare_inputs(X_l, bids)\n","\n","            results = {model: {'LogLoss': None, 'AUC': None} for model in models}\n","            for model_name in models:\n","                model = tf.keras.models.load_model(f\"{self.checkpoints_dir}/{model_name.lower()}_model_{iteration}.h5\")\n","                predictions = model.predict(inputs_for_validation).flatten()\n","                LogLoss = log_loss(y_l, predictions)\n","                AUC = roc_auc_score(y_l, predictions)\n","\n","                results[model_name]['LogLoss'] = LogLoss\n","                results[model_name]['AUC'] = AUC\n","\n","                self.results[model_name]['LogLoss'].append(LogLoss)\n","                self.results[model_name]['AUC'].append(AUC)\n","\n","            self.save_results()\n","\n","    def plot_intermediate_results(self):\n","        try:\n","            self.load_results()\n","        except:\n","            pass\n","\n","        self.plot_results()\n","        self.plot_quantile_results()\n","\n","    # Functions for quantile validation\n","    def create_filtered_dfs(self, X_l, bids, eta_l, y_l):\n","        filtered_dfs = {}\n","        for lower_quantile in range(5, 51, 5):\n","            upper_quantile = 100 - lower_quantile\n","\n","            lower_bound = np.percentile(eta_l, lower_quantile)\n","            upper_bound = np.percentile(eta_l, upper_quantile)\n","\n","            relevant_indices = np.where((eta_l <= lower_bound) | (eta_l >= upper_bound))[0]\n","\n","            filtered_dfs[f\"{lower_quantile * 2}\"] = {\n","                'X': X_l[relevant_indices],\n","                'bids': bids[relevant_indices],\n","                'y': y_l[relevant_indices]\n","            }\n","\n","        return filtered_dfs\n","\n","    def evaluate_quantiles(self, filtered_dfs, naive, ivbs, ubips):\n","        results = {\n","            'Naive': {'LogLoss': [], 'AUC': []},\n","            'IV-BS': {'LogLoss': [], 'AUC': []},\n","            'UBIPS': {'LogLoss': [], 'AUC': []}\n","        }\n","\n","        for key, data in filtered_dfs.items():\n","            X_filtered = data['X']\n","            bids_filtered = data['bids']\n","            y_filtered = data['y']\n","\n","            # Evaluate Naive model\n","            naive_logloss, naive_auc = self.evaluate_model_on_quantile(X_filtered, bids_filtered, y_filtered, naive)\n","            results['Naive']['LogLoss'].append(naive_logloss)\n","            results['Naive']['AUC'].append(naive_auc)\n","\n","            # Evaluate IVBS model\n","            ivbs_logloss, ivbs_auc = self.evaluate_model_on_quantile(X_filtered, bids_filtered, y_filtered, ivbs)\n","            results['IV-BS']['LogLoss'].append(ivbs_logloss)\n","            results['IV-BS']['AUC'].append(ivbs_auc)\n","\n","            # Evaluate UBIPS model\n","            ubips_logloss, ubips_auc = self.evaluate_model_on_quantile(X_filtered, bids_filtered, y_filtered, ubips)\n","            results['UBIPS']['LogLoss'].append(ubips_logloss)\n","            results['UBIPS']['AUC'].append(ubips_auc)\n","\n","        return results\n","\n","    def evaluate_model_on_quantile(self, X_filtered, bids_filtered, y_filtered, model):\n","        inputs_for_eval = Naive().prepare_inputs(X_filtered, bids_filtered)\n","        predictions = model.predict(inputs_for_eval)\n","        if isinstance(predictions, list):\n","            predictions = predictions[0].flatten()\n","        else:\n","            predictions = predictions.flatten()\n","\n","        LogLoss = log_loss(y_filtered, predictions)\n","        AUC = roc_auc_score(y_filtered, predictions)\n","\n","        return LogLoss, AUC\n","\n","    def plot_quantile_results(self):\n","        metrics = [\"AUC\", \"LogLoss\"]\n","        model_order = ['Naive', 'IV-BS', 'UBIPS']\n","        colors = {\n","            'Naive': '#1f77b4',\n","            'IV-BS': '#ff7f0e',\n","            'UBIPS': '#8c564b',\n","        }\n","\n","        quantiles = sorted(list(self.create_filtered_dfs(np.zeros((10, 10)), np.zeros(10), np.zeros(10), np.zeros(10)).keys()), key=lambda x: int(x))\n","\n","        for metric in metrics:\n","            fig, ax1 = plt.subplots(figsize=(18, 10))\n","            positions = np.arange(len(quantiles))\n","\n","            # Boxplot for each model\n","            for model, color in zip(model_order, colors.values()):\n","                data = [self.quantile_results[model][metric][i::len(quantiles)] for i in range(len(quantiles))]\n","                bp = ax1.boxplot(\n","                    data,\n","                    positions=positions,\n","                    widths=0.6,\n","                    patch_artist=True,\n","                    boxprops=dict(facecolor=color, color=color),\n","                    medianprops=dict(color='black'),\n","                    whiskerprops=dict(color=color),\n","                    capprops=dict(color=color),\n","                    flierprops=dict(markerfacecolor=color, markeredgecolor=color)\n","                )\n","                for box in bp['boxes']:\n","                    box.set_label(model)\n","\n","            ax1.set_xlabel(\"Outside Quantiles of $\\eta_l$ in Users' Click Response\", fontsize=20)\n","            ax1.set_ylabel(metric, fontsize=20)\n","            ax1.set_xticks(positions)\n","            ax1.set_xticklabels(quantiles, fontsize=20)\n","            ax1.tick_params(axis='y', labelsize=20)\n","\n","            # Calculate relative improvement\n","            if metric == \"AUC\":\n","                baseline = np.array([self.quantile_results['Naive'][metric][i::len(quantiles)] for i in range(len(quantiles))])\n","                improvements = {model: [] for model in model_order if model != 'Naive'}\n","                for model in improvements.keys():\n","                    model_data = np.array([self.quantile_results[model][metric][i::len(quantiles)] for i in range(len(quantiles))])\n","                    relative_improvement = ((model_data - 0.5) / (baseline - 0.5) - 1) * 100\n","                    improvements[model] = relative_improvement\n","            elif metric == \"LogLoss\":\n","                baseline = np.array([self.quantile_results['Naive'][metric][i::len(quantiles)] for i in range(len(quantiles))])\n","                improvements = {model: [] for model in model_order if model != 'Naive'}\n","                for model in improvements.keys():\n","                    model_data = np.array([self.quantile_results[model][metric][i::len(quantiles)] for i in range(len(quantiles))])\n","                    relative_improvement = (baseline - model_data) / baseline * 100\n","                    improvements[model] = relative_improvement\n","\n","            # Plot relative improvement\n","            ax2 = ax1.twinx()\n","            for model, color in zip([m for m in model_order if m != 'Naive'], [colors[m] for m in model_order if m != 'Naive']):\n","                mean_improvement = np.mean(improvements[model], axis=1)\n","                std_improvement = np.std(improvements[model], axis=1)\n","                ax2.plot(positions, mean_improvement, color=color, marker='o', label=f'Relative {model}')\n","                ax2.fill_between(positions, mean_improvement - std_improvement, mean_improvement + std_improvement, color=color, alpha=0.2)\n","            \n","            # Add Naive model's relative improvement (always 0%)\n","            ax2.axhline(0, color=colors['Naive'], linestyle='--', label='Relative Naive')\n","\n","            ax2.set_ylabel(f'Relative {metric} Improvement (%)', fontsize=20)\n","            ax2.tick_params(axis='y', labelsize=20)\n","            \n","            legend_elements = [plt.Line2D([0], [0], color=color, lw=4, label=model) for model, color in colors.items()]\n","            ax1.legend(handles=legend_elements, loc='best', fontsize=14)\n","\n","            plt.tight_layout()\n","            plt.savefig(f\"figs/sim_{metric}.pdf\", bbox_inches='tight')\n","            plt.close()\n","\n","# Usage example\n","evaluation = ModelEvaluation(replication_times=20)\n","evaluation.evaluate_model(AuctionData())\n","evaluation.plot_results()\n","evaluation.plot_quantile_results()\n","evaluation.plot_intermediate_results()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["O6QtWpncljUY"],"provenance":[]},"kernelspec":{"display_name":"Python (venv)","language":"python","name":"venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"}}},"nbformat":4,"nbformat_minor":0}
